{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29297789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidocr import RapidOCR\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import os\n",
    "import pymupdf\n",
    "\n",
    "import PIL\n",
    "\n",
    "import re\n",
    "import tabula\n",
    "from collections import Counter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_neo4j import GraphCypherQAChain, Neo4jGraph\n",
    "from langchain_neo4j import Neo4jVector\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ca9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self,api_key:str):\n",
    "        self._api_key = api_key\n",
    "        \n",
    "    def _extract_captions_of_images(self,doc,page):\n",
    "        imgs = page.get_images()\n",
    "        client = Groq(api_key=self._api_key)\n",
    "        captions = {}\n",
    "\n",
    "        for i in range(len(imgs)):\n",
    "            print(\"...VLM Called...\",end=\"\")\n",
    "\n",
    "            xref = imgs[i][0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "\n",
    "            #if image is unicolor that means it is either mask or artifact\n",
    "            if base_image['colorspace']==1:\n",
    "                continue\n",
    "\n",
    "            image_bytes = base_image[\"image\"]\n",
    "\n",
    "            image_ext = base_image[\"ext\"]\n",
    "\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image = image.resize((360,180))\n",
    "            output = io.BytesIO()\n",
    "            # image\n",
    "            image.save(output, format=image_ext)\n",
    "            base64_image = base64.b64encode(output.getvalue()).decode('utf-8')\n",
    "\n",
    "            start_time = time.time()\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": \"Describe the image in no more than 100 words as much as possible/\"},\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                                },\n",
    "                            },\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "                model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "            )\n",
    "\n",
    "            captions[f\"img_{xref}\"] = chat_completion.choices[0].message.content\n",
    "\n",
    "        return captions\n",
    "        \n",
    "    def _extract_using_ocr(self,page):\n",
    "        print(\"...OCR called...\",end=\"\")\n",
    "        img = page.get_pixmap()\n",
    "        img_bytes = img.tobytes()\n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "        if image.width > image.height:\n",
    "            image = image.rotate(90,expand=True)\n",
    "\n",
    "        image = image.resize((400,800))\n",
    "        result = engine(image)\n",
    "        text = \"\\n\".join(txt for txt in result.txts)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def _extract_text_excluding_tables(self,page):\n",
    "        tables = page.find_tables(strategy=\"lines_strict\")\n",
    "        table_bboxes = [table.bbox for table in tables]\n",
    "\n",
    "        def is_inside_any_table_bbox(bbox):\n",
    "            for table_bbox in table_bboxes:\n",
    "                # print(table_bbox)\n",
    "                if pymupdf.Rect(table_bbox).intersects(pymupdf.Rect(bbox)):\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        # Get all text blocks\n",
    "        blocks = page.get_text(\"blocks\")  \n",
    "        filtered_text = [\n",
    "            block[4] for block in blocks\n",
    "            if not is_inside_any_table_bbox(block[:4])\n",
    "        ]\n",
    "\n",
    "        return \"\\n\".join(filtered_text)\n",
    "\n",
    "    def _extract_table_content(self,page):\n",
    "        tables = page.find_tables()\n",
    "        tables_list = [table.to_markdown() for table in tables]\n",
    "\n",
    "        text = \"\\n\".join(text for text in tables_list)\n",
    "\n",
    "        return text\n",
    "    def _get_table_from_pg(self,pdf_path,pg):\n",
    "        tables = tabula.read_pdf(pdf_path,pages=str(pg+1),multiple_tables=True)\n",
    "        return tables\n",
    "    \n",
    "    def _extract_formulas_from_text(self,text):\n",
    "        formulas = []\n",
    "\n",
    "        # 1. LaTeX inline math: $...$\n",
    "        inline_latex = re.findall(r'\\$(.+?)\\$', text)\n",
    "        formulas.extend([f.strip() for f in inline_latex])\n",
    "\n",
    "        # 2. LaTeX display math: \\[...\\]\n",
    "        display_latex = re.findall(r'\\\\\\[(.+?)\\\\\\]', text, flags=re.DOTALL)\n",
    "        formulas.extend([f.strip() for f in display_latex])\n",
    "\n",
    "        # 3. LaTeX equation environments\n",
    "        env_latex = re.findall(r'\\\\begin{equation\\*?}(.+?)\\\\end{equation\\*?}', text, flags=re.DOTALL)\n",
    "        formulas.extend([f.strip() for f in env_latex])\n",
    "\n",
    "        # 4. LaTeX align environments\n",
    "        align_envs = re.findall(r'\\\\begin{align\\*?}(.+?)\\\\end{align\\*?}', text, flags=re.DOTALL)\n",
    "        formulas.extend([f.strip() for f in align_envs])\n",
    "\n",
    "        # 5. ASCII/Unicode math heuristics (e.g., x^2 + y^2 = z^2 or x² + y² = z²)\n",
    "        # Look for lines with multiple math symbols or variables\n",
    "        math_lines = []\n",
    "        for line in text.splitlines():\n",
    "            if re.search(r'[a-zA-Z0-9][\\^²³√±*/=<>+\\-]+[a-zA-Z0-9]', line):\n",
    "                if len(line.strip()) > 5:  # avoid noise\n",
    "                    math_lines.append(line.strip())\n",
    "\n",
    "        # Filter duplicates and obvious non-formulas\n",
    "        for line in math_lines:\n",
    "            if line not in formulas and not line.startswith('Figure') and '=' in line:\n",
    "                formulas.append(line)\n",
    "\n",
    "        return formulas\n",
    "    \n",
    "    \n",
    "    def _common_font_size(self,pdf_path):\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "        font_sizes = []\n",
    "\n",
    "        for page in doc:\n",
    "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            for b in blocks:\n",
    "                if \"lines\" in b:\n",
    "                    for line in b[\"lines\"]:\n",
    "                        for span in line[\"spans\"]:\n",
    "                            font_sizes.append(span[\"size\"])\n",
    "        counter = Counter(font_sizes)\n",
    "        return counter.most_common()[0][0]\n",
    "\n",
    "    def _format_headings(self,headings):\n",
    "        prev_y = 0\n",
    "        result = \"\"\n",
    "        for heading in headings:\n",
    "            if heading['bbox'][1]!=prev_y:\n",
    "                result += \"\\n\"\n",
    "            result+=heading['text']+\" \"\n",
    "            prev_y = heading['bbox'][1]\n",
    "        return result\n",
    "\n",
    "    def _get_headings(self,page,comm_font_size):\n",
    "        headings = []\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    font_size = round(span.get(\"size\", 0))\n",
    "                    font_flags = span.get(\"flags\", 0)\n",
    "                    text = span.get(\"text\", \"\").strip()\n",
    "\n",
    "                        # Skip empty strings\n",
    "                    if not text:\n",
    "                        continue\n",
    "\n",
    "                        # Heuristic: large font size is probably a heading\n",
    "                    if font_size > round(comm_font_size) or (font_size == round(comm_font_size) and (font_flags & pymupdf.TEXT_FONT_BOLD or \"Bold\" in span.get(\"font\", \"\"))):\n",
    "                        headings.append({\n",
    "                            \"text\": text,\n",
    "                            \"size\": font_size,\n",
    "                            \"font\": span.get(\"font\"),\n",
    "                            \"flags\": font_flags,\n",
    "                            \"bbox\": span.get(\"bbox\"),\n",
    "                        })\n",
    "\n",
    "        return self._format_headings(headings)\n",
    "\n",
    "\n",
    "    def parse_pdf(self,path):\n",
    "        doc = pymupdf.open(path)\n",
    "        parsed = []\n",
    "        comm_font_size = self._common_font_size(path)\n",
    "\n",
    "        for i in range(doc.page_count):\n",
    "            print(f\"Page {i+1}\",end=\"\")\n",
    "\n",
    "            full_pg = {}\n",
    "            start_time = time.time()\n",
    "            pg = doc.load_page(i)\n",
    "\n",
    "            text = self._extract_text_excluding_tables(pg)\n",
    "            img = \"\"\n",
    "            table = \"\"\n",
    "            \n",
    "            if text == \"\" or text == []:\n",
    "                text = self._extract_using_ocr(pg)\n",
    "            else:\n",
    "#                 img = self._extract_captions_of_images(doc,pg)\n",
    "#                 table = self._extract_table_content(pg)\n",
    "                table = self._get_table_from_pg(path,i)\n",
    "                headings = self._get_headings(pg,comm_font_size)\n",
    "\n",
    "            full_pg['text'] = text\n",
    "            full_pg['tables'] = table\n",
    "            full_pg['imgs'] = img\n",
    "            full_pg['page'] = i+1\n",
    "            full_pg['headings'] = headings\n",
    "            full_pg['formulas'] = self._extract_formulas_from_text(text)\n",
    "        \n",
    "            parsed.append(full_pg)\n",
    "            print(f\"..Done.. {time.time()-start_time}\")\n",
    "\n",
    "        return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38143dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(api_key=api_key)\n",
    "pdf = \"/Users/apple/Downloads/1706.03762v7.pdf\"\n",
    "result = parser.parse_pdf(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7495ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, gemini_api, groq_api, url, username, password ,database):\n",
    "        self._text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=500, chunk_overlap=100\n",
    "        )\n",
    "        self._llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            api_key=gemini_api,\n",
    "            callbacks=[StreamingStdOutCallbackHandler()]\n",
    "        )\n",
    "        self._llm_graph = LLMGraphTransformer(\n",
    "            llm=self._llm\n",
    "        )\n",
    "        self._graph =  Neo4jGraph(url=url, username=username, password=password,database=database)\n",
    "        self._graph_chain = GraphCypherQAChain.from_llm(llm=self._llm,graph = self._graph,verbose = True, allow_dangerous_requests=True)\n",
    "        self._vector_store = Neo4jVector.from_existing_index(\n",
    "            HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"),\n",
    "            url=url,\n",
    "            username=username,\n",
    "            password=password,\n",
    "            index_name=\"test\",\n",
    "            node_label=\"Chunk\",          # label you used for text chunks\n",
    "            text_node_property=\"text\",# property containing text\n",
    "            embedding_node_property=\"embedding\",\n",
    "            database=database\n",
    "        )\n",
    "        self._memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "        self._parser = Parser(api_key=groq_api)\n",
    "    \n",
    "    def parse_and_save(self, document_path):\n",
    "        result = self._parser.parse_pdf(document_path)\n",
    "        pdf = document_path\n",
    "        docs_list = [Document(page_content=page['text']+'\\n'.join(table.to_markdown() for table in page['tables'])\n",
    "                          +\"\\nHeadings: \"+ page['headings']+\n",
    "                 '\\n'.join(formula for formula in page['formulas']),\n",
    "\n",
    "                metadata={\"page\": page['page'],\"imgs\":False if not page['imgs'] \n",
    "                  else \",\".join(img.split('_')[1] for img in page['imgs']), \n",
    "                  'pdf_path':pdf,\"headings\":','.join(heading for heading in page['headings'].split('\\n'))})\n",
    "                 for page in result]\n",
    "        \n",
    "        doc_splits = self._text_splitter.split_documents(docs_list)\n",
    "        graph_documents = self._llm_graph.convert_to_graph_documents(doc_splits)\n",
    "        \n",
    "        self._graph.add_graph_documents(\n",
    "            graph_documents,\n",
    "            baseEntityLabel=True, \n",
    "            include_source=True\n",
    "        )\n",
    "        self._vector_store.add_documents(doc_splits)\n",
    "        \n",
    "    def vector_search(self,query:str)->str:\n",
    "        \"\"\"\n",
    "        Search for text similar to the query in vector database. Gives text chunk based on sentence similarity.\n",
    "        \"\"\"\n",
    "        documents = self._vector_store.similarity_search(query,k=1)\n",
    "        return \"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "    def graph_traversal(self,entity:str)->str:\n",
    "        \"\"\"\n",
    "        Search for the relationship based on the entity in query. Gives the relationship stored in knowledge graph.\n",
    "        This function is used to get relationship based data on entities.\n",
    "        \"\"\"\n",
    "        response = self._graph.query(\n",
    "                \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
    "                YIELD node,score\n",
    "                CALL {\n",
    "                  MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "                  RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "                  UNION\n",
    "                  MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "                  RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
    "                }\n",
    "                RETURN output LIMIT 50\n",
    "                \"\"\",\n",
    "                {\"query\": entity},\n",
    "            )\n",
    "        return response\n",
    "    \n",
    "    \n",
    "\n",
    "    def logical_filter(self,query):\n",
    "        \"\"\"\n",
    "        Filter out the details based on some condition on the data stored in knowledge graph.\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "            Extract structured filters from this query.\n",
    "\n",
    "            Query: {query}\n",
    "\n",
    "            And generate the Cypher query to extract the data from Neo4j\n",
    "            the keys in database are: {keys}\n",
    "\n",
    "            The data is in text key.\n",
    "\n",
    "            Strictly return the Cypher query only nothing else.\n",
    "            \"\"\")\n",
    "        keys = self._graph.query(\"CALL db.propertyKeys() YIELD propertyKey RETURN propertyKey ORDER BY propertyKey\")\n",
    "        resp = self._llm.predict(prompt.format(query=query,keys=keys))\n",
    "        if resp[:10] == \"```cypher\\n\":\n",
    "            resp =  self._graph.query(resp[10:-4])\n",
    "        else:\n",
    "            resp = self._graph.query(resp)\n",
    "        return '\\n'.join(text['n.text'] for text in resp)\n",
    "\n",
    "\n",
    "    def create_agent(self):\n",
    "        tools = [\n",
    "            Tool(name=\"VectorSearch\", func=self.vector_search, description=\"semantic match\"),\n",
    "            Tool(name=\"GraphTraversal\", func=self.graph_traversal, description=\"relationship queries\"),\n",
    "            Tool(name=\"LogicalFilter\", func=self.logical_filter, description=\"attribute filters\"),\n",
    "        ]\n",
    "\n",
    "        agent = initialize_agent(tools,self._llm,agent_type=AgentType.OPENAI_FUNCTIONS,verbose=True,memory=self._memory)\n",
    "        return agent\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b0c1aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groq_api = \"YOUR_GROQ_API\"\n",
    "gemini_api = \"YOUR_GEMINI_API\"\n",
    "\n",
    "url=\"neo4j://127.0.0.1:7687\"\n",
    "username=\"neo4j\"\n",
    "password=\"password\"\n",
    "\n",
    "\n",
    "agent_inst = Agent(gemini_api,groq_api,url,username,password,database=\"something\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7d4621c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Done.. 5.099990129470825\n",
      "Page 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Done.. 2.3515357971191406\n",
      "Page 3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Done.. 2.575409173965454\n",
      "Page 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Done.. 2.242204189300537\n"
     ]
    }
   ],
   "source": [
    "agent_inst.parse_and_save('/Users/apple/Downloads/109106172/PDF File/Week 1/The Fly - Katherine Mansfield - Comma Press.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078481d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d3a139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agent_inst.create_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7127d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: VectorSearch\n",
      "Action Input: author of The Fly\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m23/09/2019\n",
      "The Fly - Katherine Mansﬁeld - Comma Press\n",
      "\n",
      "https://commapress.co.uk/resources/online-short-stories/the-ﬂy\n",
      "1/4\n",
      "\n",
      "You are here: (https://commapress.co.uk/)\n",
      "» Understanding the Short Story (https://commapress.co.uk/resources/)\n",
      "» Online Short Stories (https://commapress.co.uk/resources/online-short-stories/)\n",
      "» The Fly - Katherine Mansﬁeld\n",
      "\n",
      "The Fly - Katherine Mansﬁeld\n",
      "\n",
      "Katherine Mansﬁeld\n",
      "\n",
      "'Y'are very snug in here,' piped old Mr. Woodiﬁeld, and peered out of the great, green-leather armchair by his\n",
      "friend the boss's desk as a baby peers out of its pram. His talk was over; it was time for him to be off. But he\n",
      "did not want to go. Since he had retired, since his ... stroke, the wife and the girls kept him boxed up in the\n",
      "house every day of the week except Tuesday. On Tuesday he was dressed and brushed and allowed to cut\n",
      "back to the City for the day. Though what he did there the wife and girls couldn't imagine. Made a nuisance of\n",
      "himself to his friends, they supposed....Well, perhaps so. All the same, we cling to our last pleasures as the tree\n",
      "clings to its last leaves. So there sat old Woodiﬁeld, smoking a cigar and staring almost greedily at the boss,\n",
      "who rolled in his ofﬁce chair, stout, rosy, ﬁve years older than he, and still going strong, still at the helm. It did\n",
      "one good to see him.\n",
      "\n",
      "Wistfully, admiringly, the old voice added, 'It's snug in here, upom my word!'\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The VectorSearch result clearly states \"The Fly - Katherine Mansfield\". Therefore, Katherine Mansfield is the author.\n",
      "Final Answer:Katherine Mansfield\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is the author of The Fly?\"\n",
    "result = agent.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c936cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e4fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
